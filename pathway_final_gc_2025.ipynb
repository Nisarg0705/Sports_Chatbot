{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noXjC8pTds0G",
        "outputId": "6cf341b9-31cc-4103-c9c2-ae24d2bcf946"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libmagic1 is already the newest version (1:5.41-3ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Requirement already satisfied: pathway[all] in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Requirement already satisfied: aiohttp>=3.8.4 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (3.11.14)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (8.1.8)\n",
            "Requirement already satisfied: geopy>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.4.1)\n",
            "Requirement already satisfied: h3>=4 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (4.2.2)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.6.1)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.0.7)\n",
            "Requirement already satisfied: sqlglot==10.6.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (10.6.1)\n",
            "Requirement already satisfied: pyarrow<19.0.0,>=10.0.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (18.1.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.32.3)\n",
            "Requirement already satisfied: python-sat>=0.1.8.dev0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.8.dev16)\n",
            "Requirement already satisfied: beartype<0.16.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.15.0)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (13.9.4)\n",
            "Requirement already satisfied: diskcache>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (5.6.3)\n",
            "Requirement already satisfied: boto3<1.36.0,>=1.26.76 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.35.99)\n",
            "Requirement already satisfied: google-api-python-client>=2.108.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.164.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (4.13.0)\n",
            "Requirement already satisfied: panel>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.6.1)\n",
            "Requirement already satisfied: jupyter-bokeh>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (4.0.5)\n",
            "Requirement already satisfied: jmespath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.0.1)\n",
            "Requirement already satisfied: aiohttp-cors>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.8.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.31.1)\n",
            "Requirement already satisfied: fs>=2.4.16 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.4.16)\n",
            "Requirement already satisfied: async-lru>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.0.5)\n",
            "Requirement already satisfied: networkx>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (3.4.2)\n",
            "Requirement already satisfied: google-cloud-pubsub>=2.21.1 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.29.0)\n",
            "Requirement already satisfied: google-cloud-bigquery~=3.29.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (3.29.0)\n",
            "Requirement already satisfied: pydantic~=2.9.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.9.2)\n",
            "Requirement already satisfied: gitpython>=3.1.43 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (3.1.44)\n",
            "Requirement already satisfied: deltalake<0.18.0,>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.17.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->pathway[all]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->pathway[all]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->pathway[all]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->pathway[all]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->pathway[all]) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->pathway[all]) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.4->pathway[all]) (1.18.3)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.99 in /usr/local/lib/python3.11/dist-packages (from boto3<1.36.0,>=1.26.76->pathway[all]) (1.35.99)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from boto3<1.36.0,>=1.26.76->pathway[all]) (0.10.4)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from deltalake<0.18.0,>=0.17.0->pathway[all]) (0.6)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.11/dist-packages (from fs>=2.4.16->pathway[all]) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from fs>=2.4.16->pathway[all]) (75.2.0)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.11/dist-packages (from fs>=2.4.16->pathway[all]) (1.17.0)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy>=2.4.0->pathway[all]) (2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython>=3.1.43->pathway[all]) (4.0.12)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=2.108.0->pathway[all]) (4.1.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery~=3.29.0->pathway[all]) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery~=3.29.0->pathway[all]) (2.7.2)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery~=3.29.0->pathway[all]) (24.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery~=3.29.0->pathway[all]) (2.8.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.51.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (1.71.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (5.29.4)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.12.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (0.14.2)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-pubsub>=2.21.1->pathway[all]) (1.71.0)\n",
            "Requirement already satisfied: bokeh==3.* in /usr/local/lib/python3.11/dist-packages (from jupyter-bokeh>=3.0.7->pathway[all]) (3.6.3)\n",
            "Requirement already satisfied: ipywidgets==8.* in /usr/local/lib/python3.11/dist-packages (from jupyter-bokeh>=3.0.7->pathway[all]) (8.1.5)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (3.1.6)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (1.3.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (2025.1.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (3.0.13)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.22.0->pathway[all]) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.22.0->pathway[all]) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.22.0->pathway[all]) (1.69.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.22.0->pathway[all]) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.31.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.22.0->pathway[all]) (1.31.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.22.0->pathway[all]) (0.52b1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->pathway[all]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->pathway[all]) (2025.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from panel>=1.3.1->pathway[all]) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.3.1->pathway[all]) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from panel>=1.3.1->pathway[all]) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from panel>=1.3.1->pathway[all]) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.11/dist-packages (from panel>=1.3.1->pathway[all]) (0.4.2)\n",
            "Requirement already satisfied: param<3.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from panel>=1.3.1->pathway[all]) (2.2.0)\n",
            "Requirement already satisfied: pyviz-comms>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from panel>=1.3.1->pathway[all]) (3.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from panel>=1.3.1->pathway[all]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.9.0->pathway[all]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic~=2.9.0->pathway[all]) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->pathway[all]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->pathway[all]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->pathway[all]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->pathway[all]) (2025.1.31)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->pathway[all]) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->pathway[all]) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->pathway[all]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->pathway[all]) (3.6.0)\n",
            "Requirement already satisfied: google-cloud-run in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.10.17)\n",
            "Requirement already satisfied: google-cloud-secret-manager in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.23.2)\n",
            "Requirement already satisfied: google-cloud-logging in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (3.11.4)\n",
            "Requirement already satisfied: docling<3.0,>=2.15 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.28.4)\n",
            "Requirement already satisfied: python-docx>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.1.2)\n",
            "Requirement already satisfied: unstructured<0.16.12,>=0.16 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.16.11)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.17.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (5.4.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (3.4.1)\n",
            "Requirement already satisfied: transformers>=4.42.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (4.50.2)\n",
            "Collecting openai~=1.60.2 (from pathway[all])\n",
            "  Using cached openai-1.60.2-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: litellm~=1.44.28 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.44.28)\n",
            "Requirement already satisfied: cohere~=5.1.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (5.1.8)\n",
            "Requirement already satisfied: tiktoken>=0.5 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.9.0)\n",
            "Collecting langchain~=0.2.0 (from pathway[all])\n",
            "  Using cached langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community~=0.2.0 (from pathway[all])\n",
            "  Using cached langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: llama-index-core~=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.10.68.post1)\n",
            "Requirement already satisfied: llama-index-readers-pathway~=0.1.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.1.3)\n",
            "Requirement already satisfied: llama-index-retrievers-pathway~=0.1.3 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.1.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (8.5.0)\n",
            "Requirement already satisfied: instructor==1.2.6 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (1.2.6)\n",
            "Requirement already satisfied: google-generativeai~=0.8.4 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (0.8.4)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor==1.2.6->pathway[all]) (0.16)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from instructor==1.2.6->pathway[all]) (0.12.5)\n",
            "Requirement already satisfied: office365-rest-python-client>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pathway[all]) (2.6.1)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere~=5.1.0->pathway[all]) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere~=5.1.0->pathway[all]) (0.28.1)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.20240311 in /usr/local/lib/python3.11/dist-packages (from cohere~=5.1.0->pathway[all]) (2.32.0.20250328)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->pathway[all]) (1.17.2)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (4.13.3)\n",
            "Requirement already satisfied: docling-core<3.0.0,>=2.24.1 in /usr/local/lib/python3.11/dist-packages (from docling-core[chunking]<3.0.0,>=2.24.1->docling<3.0,>=2.15->pathway[all]) (2.25.0)\n",
            "Requirement already satisfied: docling-ibm-models<4.0.0,>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (3.4.1)\n",
            "Requirement already satisfied: docling-parse<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (4.0.0)\n",
            "Requirement already satisfied: easyocr<2.0,>=1.7 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (1.7.2)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (1.2.0)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (0.29.3)\n",
            "Requirement already satisfied: lxml<6.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (5.3.1)\n",
            "Requirement already satisfied: marko<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (2.1.2)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (3.1.5)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (1.5.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (2.8.1)\n",
            "Requirement already satisfied: pylatexenc<3.0,>=2.10 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (2.10)\n",
            "Requirement already satisfied: pypdfium2<5.0.0,>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (4.30.1)\n",
            "Requirement already satisfied: python-pptx<2.0.0,>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (1.0.2)\n",
            "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from docling<3.0,>=2.15->pathway[all]) (1.4.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.43->pathway[all]) (5.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=2.108.0->pathway[all]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=2.108.0->pathway[all]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=2.108.0->pathway[all]) (4.9)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.11/dist-packages (from google-generativeai~=0.8.4->pathway[all]) (0.6.15)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-resumable-media<3.0dev,>=2.0.0->google-cloud-bigquery~=3.29.0->pathway[all]) (1.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client>=2.108.0->pathway[all]) (3.2.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.22.0->pathway[all]) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=2.9->bokeh==3.*->jupyter-bokeh>=3.0.7->pathway[all]) (3.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain~=0.2.0->pathway[all]) (2.0.40)\n",
            "Collecting langchain-core<0.3.0,>=0.2.43 (from langchain~=0.2.0->pathway[all])\n",
            "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain~=0.2.0->pathway[all])\n",
            "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain~=0.2.0->pathway[all]) (0.1.147)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community~=0.2.0->pathway[all]) (0.6.7)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm~=1.44.28->pathway[all]) (4.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm~=1.44.28->pathway[all]) (1.1.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm~=1.44.28->pathway[all]) (0.21.1)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core~=0.10.0->pathway[all]) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core~=0.10.0->pathway[all]) (2025.3.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core~=0.10.0->pathway[all]) (1.6.0)\n",
            "Requirement already satisfied: nltk!=3.9,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core~=0.10.0->pathway[all]) (3.9.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core~=0.10.0->pathway[all]) (0.9.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py->panel>=1.3.1->pathway[all]) (0.1.2)\n",
            "Requirement already satisfied: msal in /usr/local/lib/python3.11/dist-packages (from office365-rest-python-client>=2.5.3->pathway[all]) (1.32.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.60.2->pathway[all]) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.60.2->pathway[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai~=1.60.2->pathway[all]) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai~=1.60.2->pathway[all]) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5->pathway[all]) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0->pathway[all]) (3.18.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.42.0->pathway[all]) (0.5.3)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (5.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (0.4.27)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (2.14.1)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (3.12.2)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (2.2.1)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (0.28.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured<0.16.12,>=0.16->pathway[all]) (1.1)\n",
            "Requirement already satisfied: unstructured-inference==0.8.1 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.8.1)\n",
            "Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (3.10.1)\n",
            "Requirement already satisfied: pi-heif in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.22.0)\n",
            "Requirement already satisfied: pikepdf in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (9.5.2)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (20250327)\n",
            "Requirement already satisfied: effdet in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.4.1)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (2.0.1)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.17.0)\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.15)\n",
            "Requirement already satisfied: unstructured.pytesseract>=0.3.12 in /usr/local/lib/python3.11/dist-packages (from unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.3.15)\n",
            "Requirement already satisfied: layoutparser in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.3.4)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.0.20)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (4.11.0.86)\n",
            "Requirement already satisfied: onnxruntime>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.21.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (2.6.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.0.15)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->panel>=1.3.1->pathway[all]) (0.5.1)\n",
            "Requirement already satisfied: google-cloud-appengine-logging<2.0.0dev,>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-logging->pathway[all]) (1.6.1)\n",
            "Requirement already satisfied: google-cloud-audit-log<1.0.0dev,>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-logging->pathway[all]) (0.3.2)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.11/dist-packages (from linkify-it-py->panel>=1.3.1->pathway[all]) (1.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling<3.0,>=2.15->pathway[all]) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community~=0.2.0->pathway[all]) (3.26.1)\n",
            "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling<3.0,>=2.15->pathway[all]) (1.1.0)\n",
            "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling<3.0,>=2.15->pathway[all]) (3.77.0)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from docling-core<3.0.0,>=2.24.1->docling-core[chunking]<3.0.0,>=2.24.1->docling<3.0,>=2.15->pathway[all]) (0.9.0)\n",
            "Requirement already satisfied: semchunk<3.0.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from docling-core[chunking]<3.0.0,>=2.24.1->docling<3.0,>=2.15->pathway[all]) (2.2.2)\n",
            "Requirement already satisfied: jsonlines<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling<3.0,>=2.15->pathway[all]) (3.1.0)\n",
            "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.6.0.66 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling<3.0,>=2.15->pathway[all]) (4.11.0.86)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.11/dist-packages (from docling-ibm-models<4.0.0,>=3.4.0->docling<3.0,>=2.15->pathway[all]) (0.21.0+cu124)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling<3.0,>=2.15->pathway[all]) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling<3.0,>=2.15->pathway[all]) (0.6.6)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling<3.0,>=2.15->pathway[all]) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from easyocr<2.0,>=1.7->docling<3.0,>=2.15->pathway[all]) (1.11.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere~=5.1.0->pathway[all]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere~=5.1.0->pathway[all]) (0.14.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (3.0.50)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (4.9.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm~=1.44.28->pathway[all]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm~=1.44.28->pathway[all]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm~=1.44.28->pathway[all]) (0.24.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.43->langchain~=0.2.0->pathway[all]) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain~=0.2.0->pathway[all]) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain~=0.2.0->pathway[all]) (1.0.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling<3.0,>=2.15->pathway[all]) (2.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=2.108.0->pathway[all]) (0.6.1)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from python-pptx<2.0.0,>=1.0.2->docling<3.0,>=2.15->pathway[all]) (3.2.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain~=0.2.0->pathway[all]) (3.1.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.9.0->instructor==1.2.6->pathway[all]) (1.5.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core~=0.10.0->pathway[all]) (1.0.0)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (2.0.8)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (2.3.0)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal->office365-rest-python-client>=2.5.3->pathway[all]) (2.10.1)\n",
            "Requirement already satisfied: cryptography<47,>=2.5 in /usr/local/lib/python3.11/dist-packages (from msal->office365-rest-python-client>=2.5.3->pathway[all]) (43.0.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured<0.16.12,>=0.16->pathway[all]) (0.47)\n",
            "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured<0.16.12,>=0.16->pathway[all])\n",
            "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured<0.16.12,>=0.16->pathway[all]) (0.2.2)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured<0.16.12,>=0.16->pathway[all]) (1.0.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<47,>=2.5->msal->office365-rest-python-client>=2.5.3->pathway[all]) (1.17.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.8.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain~=0.2.0->pathway[all]) (3.0.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (4.9.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (25.2.10)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets==8.*->jupyter-bokeh>=3.0.7->pathway[all]) (0.2.13)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (1.4.8)\n",
            "Requirement already satisfied: mpire[dill] in /usr/local/lib/python3.11/dist-packages (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.24.1->docling<3.0,>=2.15->pathway[all]) (2.10.2)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.11/dist-packages (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.1.10)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (0.11.6)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling<3.0,>=2.15->pathway[all]) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling<3.0,>=2.15->pathway[all]) (2025.3.13)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr<2.0,>=1.7->docling<3.0,>=2.15->pathway[all]) (0.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<47,>=2.5->msal->office365-rest-python-client>=2.5.3->pathway[all]) (2.22)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (10.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.11/dist-packages (from iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]<0.16.15,>=0.16; extra == \"xpack-llm-local\"->pathway[all]) (3.1.1)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.11/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.24.1->docling<3.0,>=2.15->pathway[all]) (0.70.17)\n",
            "Requirement already satisfied: dill>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.24.1->docling<3.0,>=2.15->pathway[all]) (0.3.9)\n",
            "Using cached langchain-0.2.17-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_community-0.2.19-py3-none-any.whl (2.3 MB)\n",
            "Using cached openai-1.60.2-py3-none-any.whl (456 kB)\n",
            "Using cached langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "Using cached langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: aiofiles, openai, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 23.2.1\n",
            "    Uninstalling aiofiles-23.2.1:\n",
            "      Successfully uninstalled aiofiles-23.2.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.68.2\n",
            "    Uninstalling openai-1.68.2:\n",
            "      Successfully uninstalled openai-1.68.2\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.49\n",
            "    Uninstalling langchain-core-0.3.49:\n",
            "      Successfully uninstalled langchain-core-0.3.49\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.7\n",
            "    Uninstalling langchain-text-splitters-0.3.7:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.7\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.22\n",
            "    Uninstalling langchain-0.3.22:\n",
            "      Successfully uninstalled langchain-0.3.22\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.3.20\n",
            "    Uninstalling langchain-community-0.3.20:\n",
            "      Successfully uninstalled langchain-community-0.3.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.3.11 requires langchain-core<1.0.0,>=0.3.49, but you have langchain-core 0.2.43 which is incompatible.\n",
            "langchain-openai 0.3.11 requires openai<2.0.0,>=1.68.2, but you have openai 1.60.2 which is incompatible.\n",
            "gradio 5.23.2 requires aiofiles<24.0,>=22.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 langchain-0.2.17 langchain-community-0.2.19 langchain-core-0.2.43 langchain-text-splitters-0.2.4 openai-1.60.2\n",
            "Collecting openai==1.68.2\n",
            "  Using cached openai-1.68.2-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.68.2) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.68.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.68.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.68.2) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai==1.68.2) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai==1.68.2) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai==1.68.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai==1.68.2) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai==1.68.2) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.68.2) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai==1.68.2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.68.2) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.68.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai==1.68.2) (2.23.4)\n",
            "Using cached openai-1.68.2-py3-none-any.whl (606 kB)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.60.2\n",
            "    Uninstalling openai-1.60.2:\n",
            "      Successfully uninstalled openai-1.60.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-openai 0.3.11 requires langchain-core<1.0.0,>=0.3.49, but you have langchain-core 0.2.43 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-1.68.2\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.2.19)\n",
            "Collecting langchain-community\n",
            "  Using cached langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.11/dist-packages (0.1.21)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.11)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.23.2)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.2.43)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.23)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.60)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Collecting langchain-core<0.4,>=0.1 (from langgraph)\n",
            "  Using cached langchain_core-0.3.49-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain<1.0.0,>=0.3.21 (from langchain-community)\n",
            "  Using cached langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.1.147)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (2.32.0.20250328)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.68.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.7 (from langchain<1.0.0,>=0.3.21->langchain-community)\n",
            "  Using cached langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Using cached langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Using cached langchain-0.3.22-py3-none-any.whl (1.0 MB)\n",
            "Using cached langchain_core-0.3.49-py3-none-any.whl (420 kB)\n",
            "Using cached langchain_text_splitters-0.3.7-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: aiofiles, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.43\n",
            "    Uninstalling langchain-core-0.2.43:\n",
            "      Successfully uninstalled langchain-core-0.2.43\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.4\n",
            "    Uninstalling langchain-text-splitters-0.2.4:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.4\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.17\n",
            "    Uninstalling langchain-0.2.17:\n",
            "      Successfully uninstalled langchain-0.2.17\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.2.19\n",
            "    Uninstalling langchain-community-0.2.19:\n",
            "      Successfully uninstalled langchain-community-0.2.19\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unstructured-client 0.28.1 requires aiofiles>=24.1.0, but you have aiofiles 23.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 langchain-0.3.22 langchain-community-0.3.20 langchain-core-0.3.49 langchain-text-splitters-0.3.7\n"
          ]
        }
      ],
      "source": [
        "!apt-get install libmagic1\n",
        "!pip install -U \"pathway[all]\"\n",
        "!pip install openai==1.68.2\n",
        "!pip install -U langgraph langchain-community langchainhub langchain-openai gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ff9da9f-66b8-4675-8c45-afa10e891462"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "import time\n",
        "import json\n",
        "from typing import Iterable, Literal, List\n",
        "from pydantic import BaseModel, Field\n",
        "import pathway as pw\n",
        "from langchain_community.vectorstores import PathwayVectorClient\n",
        "from pathway.xpacks.llm.vector_store import VectorStoreServer, VectorStoreClient\n",
        "from pathway.xpacks.llm import (\n",
        "    embedders,\n",
        "    llms,\n",
        "    parsers,\n",
        "    splitters,\n",
        ")\n",
        "from pathway.udfs import DiskCache\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# needed for the OpenAI embedder and the LLM we will use below, you can change the embedding provider, see the documentation:\n",
        "# https://pathway.com/developers/api-docs/pathway-xpacks-llm/embedders\n",
        "# days='1'\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "l-Erh2GSe_cK"
      },
      "outputs": [],
      "source": [
        "# folder that we will gather our docs in\n",
        "DATA_PATH = \"./data\"\n",
        "\n",
        "os.makedirs(DATA_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "aSG2G76ZfB2H"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from urllib.parse import urlparse\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "\n",
        "def load_page_content(url: str) -> str:\n",
        "    \"\"\"Load web page content with Langchain utilities.\"\"\"\n",
        "    return WebBaseLoader(url).load()[0].page_content\n",
        "\n",
        "\n",
        "def ingest_webpage(url: str) -> None:\n",
        "    \"\"\"Save a webpage to local `DATA_PATH` folder.\"\"\"\n",
        "    text_content = load_page_content(url)\n",
        "\n",
        "    parsed_url = urlparse(url)\n",
        "    file_name = parsed_url.hostname + parsed_url.path.replace(\"/\", \"_\") + \".txt\"\n",
        "\n",
        "    with open(os.path.join(DATA_PATH, file_name), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text_content)\n",
        "\n",
        "def ingest_text_file(text_content: str, name: str) -> None:\n",
        "    \"\"\"Save a text file to local `DATA_PATH` folder.\"\"\"\n",
        "    # Using index to create unique and short filenames\n",
        "    file_name = f\"scraped_data_{name}.txt\"\n",
        "\n",
        "    with open(os.path.join(DATA_PATH, file_name), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(text_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "wnaszlgQftbE"
      },
      "outputs": [],
      "source": [
        "# ingest_webpage(\"https://www.espncricinfo.com/cricketers/virat-kohli-253802\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "J8S5C8eTfo5P"
      },
      "outputs": [],
      "source": [
        "# host and port of the RAG app\n",
        "pathway_host: str = \"0.0.0.0\"\n",
        "pathway_port: int = 8000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "JX1uOwAlgEdy"
      },
      "outputs": [],
      "source": [
        "# user_input = input(\"Ask something: \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GezPzc5W5orU"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Replace these with your credentials\n",
        "GOOGLE_API_KEY = \"Google_API_Key\"\n",
        "\n",
        "OPENAI_API_KEY = \"Open_AI_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "R4sssR4QN9UU"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenAI client\n",
        "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))  # Ensure API key is set as an environment variable\n",
        "\n",
        "def query_constructor(query):\n",
        "    system_prompt = '''You are an agent that specializes in sports knowledge. Modify the query to make it concise and effective for web search.\n",
        "    The query should be modified in such a way that it give precise and accurate search result on google web search\n",
        "    For example if the query is \"what are the stats of virat kohli in t20\"\n",
        "    the response should be \"Virat Kohli T20 stats latest\"\n",
        "    '''\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"Make this query concise for web search: {query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content + \" -site:x.com -site:youtube.com -site:reddit.com -site:instagram.com -site:facebook.com -site:linkedin.com -site:thread.net \"\n",
        "\n",
        "# Example usage\n",
        "# query = \"virat kohli t20 stats\"\n",
        "# print(query_constructor(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBvb2JanMZgC"
      },
      "outputs": [],
      "source": [
        "CX_ID = \"CX_ID\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "5gOMY93c5pvJ"
      },
      "outputs": [],
      "source": [
        "def google_search(query,search_days):\n",
        "    \"\"\"Fetch search results from Google Custom Search API\"\"\"\n",
        "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={GOOGLE_API_KEY}&cx={CX_ID}&dateRestrict=d{search_days}\"\n",
        "    print(url)\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    results = response.json()\n",
        "    print(results)\n",
        "\n",
        "    snippets = []\n",
        "    links=[]\n",
        "    for item in results.get(\"items\", []):\n",
        "        snippets.append(item['snippet'])\n",
        "        links.append(item['link'])\n",
        "    return links,snippets  # Return top 10 search results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "VhJwTNl2-acE"
      },
      "outputs": [],
      "source": [
        "import pathway as pw\n",
        "from pathway.stdlib.indexing import BruteForceKnnFactory, HybridIndexFactory\n",
        "from pathway.stdlib.indexing.bm25 import TantivyBM25Factory\n",
        "from pathway.udfs import DiskCache\n",
        "from pathway.xpacks.llm import embedders, llms, parsers, splitters\n",
        "from pathway.xpacks.llm.document_store import DocumentStore\n",
        "from pathway.xpacks.llm.question_answering import BaseRAGQuestionAnswerer, RAGClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "AOok4-eF8pBl"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import time\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "g4KeIcDaeST6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
        "    If it is relevant to  the question to some extent also then also give yes\n",
        "    give no in very very harsh scenario\n",
        "    \"\"\"\n",
        "\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "dW8_mnBmgEfx"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GX9pUp96hGF"
      },
      "source": [
        "\n",
        "**AGENTIC RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "jHUHidTWi2z3"
      },
      "outputs": [],
      "source": [
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts or the text has some facts that can prove it.\n",
        "     Dont be too harsh and give no in very very harsh scenario otherwise ouput yes in almost every scenario\n",
        "     \"\"\"\n",
        "\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "# hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "AEqDhx5K0aKF"
      },
      "outputs": [],
      "source": [
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "# answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Obvv8mg_C_sv"
      },
      "outputs": [],
      "source": [
        "#Function for transformaing the question into better prompt\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "\n",
        "def query_rewritter(question):\n",
        "  # Prompt\n",
        "  system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "      for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\n",
        "      For example if the query: What are the T20 statistics for Virat Kohli?\n",
        "      Your response should be :  Virat Kohli t20 stats\n",
        "      Use short words whenever possible like use  stats insted of statistics. Use HS instead of Highest Score and avg instead of Average\n",
        "\n",
        "      Your response should be such that it is easy to retreive in the vector space. make it as short and crisp as possible\n",
        "      \"\"\"\n",
        "  re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "      [\n",
        "          (\"system\", system),\n",
        "          (\n",
        "              \"human\",\n",
        "              \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "          ),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  query_rewriter_agent = re_write_prompt | llm | StrOutputParser()\n",
        "  return query_rewriter_agent.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "34aTe7g10zAt"
      },
      "outputs": [],
      "source": [
        "#For transformation of question into better question\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "# question_rewriter.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "O5yo4sN804mS"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    input_days: str\n",
        "    generation: str\n",
        "    documents: List[str]\n",
        "    commentary: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "kHLh6V56x-fL"
      },
      "outputs": [],
      "source": [
        "def query_answer(query):\n",
        "    system_prompt = '''\n",
        "    You are an AI agent specializing in sports knowledge. Your task is to determine whether a given query can be answered based on your existing knowledge.\n",
        "\n",
        "    Response Criteria:\n",
        "    - Respond **\"yes\"** if the query pertains to **objective and factual sports knowledge** **within your knowledge cutoff (October 2023)**, such as:\n",
        "      - Historical events, past tournaments, and iconic matches that occurred **on or before October 2023**.\n",
        "      - Established rules, gameplay mechanics, and strategies.\n",
        "      - Retired players, records, and achievements that **are permanently fixed**.\n",
        "\n",
        "    - Respond **\"no\"** if the query:\n",
        "      - Requires **real-time or frequently changing information**, including:\n",
        "        - Live scores, current player statistics, recent rankings, or ongoing tournaments.\n",
        "        - Any record held by an **active player (as of October 2023)**, since they can break their own records.\n",
        "          - Example: *\"What is the highest score of Rohit Sharma in IPL?\"  \"no\"*\n",
        "      - Is **opinion-based, subjective, or debatable**, such as:\n",
        "        - *\"Who is the greatest footballer of all time?\"*\n",
        "        - *\"Which team has the best fan base?\"*\n",
        "      - Requires speculation, personal preference, or future predictions.\n",
        "\n",
        "    Your response should be strictly **\"yes\"** or **\"no\"**nothing else.\n",
        "\n",
        "    '''\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"{query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "4JmyHeRCdDI8"
      },
      "outputs": [],
      "source": [
        "# Change the prompt\n",
        "\n",
        "\n",
        "def query_type_answer(query):\n",
        "    system_prompt = '''\n",
        "    You are an AI agent specializing in sports knowledge. Your task is to determine whether a given query is asking for live commentary or not\n",
        "    If it is asking for live commentary then respond with **\"yes\"** else **\"no\"**\n",
        "    Even if it is asking for updates say no there should live commentary written in the prompt\n",
        "    Your response should be strictly **\"yes\"** or **\"no\"**nothing else.\n",
        "\n",
        "    '''\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"{query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "06nEuO1zpjHL"
      },
      "outputs": [],
      "source": [
        "def query_type_sport(query):\n",
        "    system_prompt = '''\n",
        "    You are an AI agent specializing in sports knowledge.Your task is to categorize about which sports is the query talking about\n",
        "    You have to choose from ['football','cricket','basketball']\n",
        "    Output just the name of the sport\n",
        "\n",
        "    Your response should be strictly **\"football\"** or **\"cricket\"** or **\"basketball\"**nothing else.\n",
        "    '''\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"{query}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "oF78R1IzeKm0"
      },
      "outputs": [],
      "source": [
        "def commentary_agent(query):\n",
        "  question=query['question']\n",
        "  answer=query_type_sport(question)\n",
        "  if answer=='football':\n",
        "    links,snippets=google_search(query_constructor('site:https://www.sofascore.com/football/match/ '+ question),'1')\n",
        "    link_choosen=commentary_link_assignment(question,links)\n",
        "  elif answer=='cricket':\n",
        "    links,snippets=google_search('site:www.cricbuzz.com/live-cricket-scores/ '+ question,'1')\n",
        "    link_choosen=commentary_link_assignment(question,links)\n",
        "  else:\n",
        "    links,snippets=google_search(query_constructor('site:https://www.espn.com/ '+ question),'1')\n",
        "    link_choosen=commentary_link_assignment(question,links)\n",
        "  while True:\n",
        "    print(link_choosen)\n",
        "    live_commentary(link_choosen,3)\n",
        "    generated = summary_agent('/content/data/diff.txt',question)\n",
        "    return {\"question\": question, \"generation\": generated , \"commentary\":1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "QI4jPutvgpmk"
      },
      "outputs": [],
      "source": [
        "# def commentary_agent(query):\n",
        "#   question=query['question']\n",
        "#   return {\"question\": question,\"commentary\":1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "S-RFIUBKF6RP"
      },
      "outputs": [],
      "source": [
        "def llm_agent(query):\n",
        "    question=query['question']\n",
        "    system_prompt = '''\n",
        "    You are an AI agent specializing in sports knowledge. Your task is to provide the answer to the query\n",
        "    '''\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": f\"{question}\"}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "    return {\"question\": question, \"generation\": response.choices[0].message.content}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "TK2b3O4-H3bE"
      },
      "outputs": [],
      "source": [
        "def agent_decision(state):\n",
        "  if query_type_answer(state[\"question\"])=='yes':\n",
        "    print(\"Going to commentary_agent\")\n",
        "    return \"commentary_agent\"\n",
        "  else:\n",
        "    if query_answer(state[\"question\"])=='yes':\n",
        "      print(\"Going to llm_agent\")\n",
        "      return \"llm_agent\"\n",
        "    else:\n",
        "      print(\"Going to data_creation_and_scarpping\")\n",
        "      return \"data_creation_and_scarpping\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "SeAUqPo707ss"
      },
      "outputs": [],
      "source": [
        "def data_creation_and_scarpping(state):\n",
        "  question=state[\"question\"]\n",
        "  search_days=state[\"input_days\"]\n",
        "  links,snippets=google_search(query_constructor(question),str(search_days))\n",
        "  print(google_search(query_constructor(question),str(search_days)))\n",
        "  while len(links)==0:\n",
        "    print(\"I am here\")\n",
        "    links,snippets=google_search(query_constructor(question),str(search_days))\n",
        "  i=1\n",
        "  for link in links[:7]:\n",
        "    print(link)\n",
        "    payload = { 'api_key': 'd80856c1689fbd8802e1dd90fd5f4e2c', 'url': f'{link}', 'device_type': 'desktop' }\n",
        "    r = requests.get('https://api.scraperapi.com/', params=payload)\n",
        "    # print(r.text)\n",
        "    ingest_text_file(str(r.text),str(i))\n",
        "    i+=1\n",
        "  folder = pw.io.fs.read(\n",
        "    path=f\"{DATA_PATH}/*.txt\",\n",
        "    format=\"binary\",\n",
        "    with_metadata=True,\n",
        "\n",
        "  )\n",
        "\n",
        "  # list of data sources to be indexed\n",
        "  sources = [folder]\n",
        "\n",
        "  # define the document processing steps\n",
        "  parser = parsers.UnstructuredParser()\n",
        "\n",
        "  text_splitter = splitters.TokenCountSplitter(min_tokens=150, max_tokens=450)\n",
        "\n",
        "  embedder = embedders.OpenAIEmbedder(cache_strategy=DiskCache())\n",
        "\n",
        "  vector_server = VectorStoreServer(\n",
        "      *sources,\n",
        "      embedder=embedder,\n",
        "      splitter=text_splitter,\n",
        "      parser=parser,\n",
        "  )\n",
        "\n",
        "  t = vector_server.run_server(pathway_host, pathway_port, threaded=True)\n",
        "\n",
        "\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    # pathway_client = VectorStoreClient(pathway_host, pathway_port)\n",
        "    vectorstore_client = PathwayVectorClient(pathway_host, pathway_port)\n",
        "    retriever = vectorstore_client.as_retriever()\n",
        "    max_retries = 15  # Number of retries\n",
        "    retry_delay = 5  # Seconds to wait before retrying\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            docs = retriever.invoke({\"question\": query_rewritter(question)})  # No timeout param\n",
        "            break  # Exit loop if successful\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "            time.sleep(retry_delay)\n",
        "    else:\n",
        "        print(\"Failed after multiple attempts.\")\n",
        "\n",
        "\n",
        "    print(query_rewritter(question))\n",
        "    return {\"documents\": docs, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    print(question)\n",
        "    print()\n",
        "    # RAG generation\n",
        "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "    rag_chain = prompt | llm | StrOutputParser()\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        # grade='yes'\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "    # Check hallucination\n",
        "    # grade='yes'\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        # grade='yes'\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "4oDSCX3j1Bqv"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"commentary_agent\",commentary_agent)\n",
        "workflow.add_node(\"agent_decision\",agent_decision)\n",
        "workflow.add_node(\"llm_agent\",llm_agent)\n",
        "workflow.add_node(\"data_creation_and_scarpping\",data_creation_and_scarpping)\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generate\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "\n",
        "# Build graph\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    agent_decision,\n",
        "     {\n",
        "         \"llm_agent\" : \"llm_agent\",\n",
        "         \"commentary_agent\" : \"commentary_agent\",\n",
        "         \"data_creation_and_scarpping\" : \"data_creation_and_scarpping\",\n",
        "     },\n",
        ")\n",
        "workflow.add_edge(\"commentary_agent\", END)\n",
        "workflow.add_edge(\"llm_agent\", END)\n",
        "workflow.add_edge(\"data_creation_and_scarpping\", \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"transform_query\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "k1PgUAahTkC6"
      },
      "outputs": [],
      "source": [
        "def commentary_link_assignment(question,links):\n",
        "    system_prompt = f'''\n",
        "    You are an AI agent specializing in sports knowledge. You will be given a list of links and a query.\n",
        "    Your task is to output the most suitable link corresponding to that query\n",
        "    just output the link and nothing else.\n",
        "\n",
        "    Here are the links:\n",
        "    {links}\n",
        "\n",
        "   '''\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "q6CYtCRmYnyG"
      },
      "outputs": [],
      "source": [
        "# # For Internal Testing\n",
        "\n",
        "# inputs = {\n",
        "#     \"question\": \"mi vs kkr Live Commentary 2025\",\n",
        "#     \"input_days\" : \"1\"\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "9SifJCxUsvrw"
      },
      "outputs": [],
      "source": [
        "# links,snippet=google_search(query_constructor('site:https://www.sofascore.com/football/match/ '+ \"Torino vs Lazio Live Commentary latest\"),'1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Y_rLvMm4XK_N"
      },
      "outputs": [],
      "source": [
        "# for link in links:\n",
        "#   print(link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "uRa87QDX1N1D"
      },
      "outputs": [],
      "source": [
        "# result = app.invoke(inputs)\n",
        "# if result[\"commentary\"]==1:\n",
        "\n",
        "# result[\"generation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "bNBc-WqpOVd9"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "def summary_agent(txt_file, question, model=\"gpt-4o\", batch_size=100000): # Add question as a parameter\n",
        "    enc = tiktoken.encoding_for_model(model)\n",
        "\n",
        "    # Read entire file\n",
        "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    # Tokenize the whole document\n",
        "    tokenized_data = enc.encode(data)\n",
        "\n",
        "    # Split into batches of 100,000 tokens\n",
        "    batches = [tokenized_data[i : i + batch_size] for i in range(0, len(tokenized_data), batch_size)]\n",
        "\n",
        "    batch_summaries = []\n",
        "    for i, batch in enumerate(batches):\n",
        "        truncated_data = enc.decode(batch)  # Convert back to string\n",
        "\n",
        "        system_prompt = f'''\n",
        "        You are an AI agent specializing in sports knowledge. You will be given a txt file containing updates.\n",
        "        Your task is to provide a summary of the updates\n",
        "\n",
        "        Here is batch {i + 1}:\n",
        "        {truncated_data}\n",
        "        '''\n",
        "\n",
        "        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "\n",
        "        batch_summaries.append(response.choices[0].message.content)\n",
        "\n",
        "    # Combine batch summaries into a final summary\n",
        "    combined_summary_prompt = f'''\n",
        "    You have been given multiple batch summaries of a large document. Your task is to generate a concise Commentary of the updates on match referred in \"{question}\".\n",
        "    You dont have to summarise the whole match just summarise the updates\n",
        "    Here are the batch summaries:\n",
        "    {batch_summaries}\n",
        "    '''\n",
        "\n",
        "    final_messages = [{\"role\": \"system\", \"content\": combined_summary_prompt}]\n",
        "\n",
        "    final_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=final_messages\n",
        "    )\n",
        "\n",
        "    return final_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "00xWMIFp95hw"
      },
      "outputs": [],
      "source": [
        "import difflib\n",
        "import requests\n",
        "import time\n",
        "def live_commentary(url,sleep_duration):\n",
        "\n",
        "  payload = { 'api_key': 'd80856c1689fbd8802e1dd90fd5f4e2c', 'url': f'{url}', 'device_type': 'desktop' }\n",
        "  r = requests.get('https://api.scraperapi.com/', params=payload)\n",
        "  ingest_text_file(str(r.text),'old')\n",
        "  time.sleep(sleep_duration)\n",
        "  r = requests.get('https://api.scraperapi.com/', params=payload)\n",
        "  ingest_text_file(str(r.text),'new')\n",
        "\n",
        "  # Load the scraped data (assuming it's stored as text)\n",
        "  with open(\"/content/data/scraped_data_old.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "      old_data = f.readlines()\n",
        "\n",
        "  with open(\"/content/data/scraped_data_new.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "      new_data = f.readlines()\n",
        "\n",
        "  # Compare the two versions\n",
        "  diff = difflib.unified_diff(old_data, new_data, lineterm=\"\")\n",
        "\n",
        "  lines=[]\n",
        "  # Print only the added lines\n",
        "  for line in diff:\n",
        "      if line.startswith(\"+\") and not line.startswith(\"+++\"):\n",
        "          lines.append(line)\n",
        "  with open(\"/content/data/diff.txt\", \"w\") as file:\n",
        "    file.write('\\n'.join(lines))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrwnqPOWnvgs"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from langgraph.graph import StateGraph\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import time  # For typing effect\n",
        "\n",
        "# Function to invoke LangGraph and maintain chat history\n",
        "def app_invoke(question, chat_history, number_input):\n",
        "    if chat_history is None:\n",
        "        chat_history = []  # Initialize history\n",
        "\n",
        "    # Add user message\n",
        "    chat_history.append((\" You\", question))\n",
        "\n",
        "    # Simulating a typing effect\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    if app is None:\n",
        "        response = \"Error: LangGraph app is not initialized.\"\n",
        "    else:\n",
        "        # Run the LangGraph workflow\n",
        "        input_data = {\"question\": question, \"history\": chat_history, \"input_days\": number_input, \"commentary\":0}\n",
        "        result = app.invoke(input_data)\n",
        "\n",
        "        # Extract response\n",
        "        response = result.get(\"generation\", \"No response generated.\")\n",
        "\n",
        "    # Add assistant response\n",
        "    chat_history.append((\" AI\", response))\n",
        "\n",
        "    return chat_history, chat_history  # Return updated state\n",
        "\n",
        "# Define Gradio UI\n",
        "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"blue\")) as interface:\n",
        "    gr.HTML(\"<h1 style='text-align: center; color: #00aaff;'> AI-Powered Sports Chatbot</h1>\")\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        label=\"Chat History\",\n",
        "        bubble_full_width=False  # Ensures messages appear neatly\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        number_input = gr.Number(label=\"Enter a number\", interactive=True)\n",
        "        user_input = gr.Textbox(\n",
        "            label=\"Type your question...\",\n",
        "            placeholder=\"Ask me about any sport! \",\n",
        "            interactive=True\n",
        "        )\n",
        "        submit_btn = gr.Button(\" Send\", variant=\"primary\")\n",
        "\n",
        "    with gr.Row():\n",
        "        clear_btn = gr.Button(\" Clear Chat\", variant=\"stop\")\n",
        "\n",
        "    state = gr.State(value=[])  # Chat history storage\n",
        "\n",
        "    # Define button click behavior\n",
        "    def submit_question(user_text, number_value, chat_history):\n",
        "        return app_invoke(user_text, chat_history, number_value)\n",
        "\n",
        "    user_input.submit(submit_question, [user_input, number_input, state], [chatbot, state])\n",
        "    submit_btn.click(submit_question, [user_input, number_input, state], [chatbot, state])\n",
        "\n",
        "    def clear_chat():\n",
        "        return [], []\n",
        "\n",
        "    clear_btn.click(clear_chat, [], [chatbot, state])\n",
        "\n",
        "# Launch Gradio UI\n",
        "interface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQdJwdzKCzPc"
      },
      "outputs": [],
      "source": [
        "#Live Commentary\n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# Generator function to simulate continuous commentary\n",
        "def commentary_classify(user_input):\n",
        "    question = user_input\n",
        "    answer = query_type_sport(question)\n",
        "    if answer == 'football':\n",
        "        links, snippets = google_search(query_constructor('site:https://www.sofascore.com/football/match/ ' + question), '1')\n",
        "        link_choosen = commentary_link_assignment(question, links)\n",
        "    elif answer == 'cricket':\n",
        "        links, snippets = google_search('site:www.cricbuzz.com/live-cricket-scores/ ' + question, '1')\n",
        "        link_choosen = commentary_link_assignment(question, links)\n",
        "    else:\n",
        "        links, snippets = google_search(query_constructor('site:https://www.espn.com/ ' + question), '1')\n",
        "        link_choosen = commentary_link_assignment(question, links)\n",
        "    return link_choosen\n",
        "\n",
        "# Generator function for continuous commentary in chat format (without user text)\n",
        "def continuous_commentary(user_input):\n",
        "    question = user_input\n",
        "    chat_history = []\n",
        "    if user_input:\n",
        "        while True:\n",
        "            live_commentary(commentary_classify(user_input), 3)\n",
        "            generated = summary_agent('/content/data/diff.txt', question)\n",
        "            chat_history.append((None, f'Bot: {generated}'))\n",
        "            yield chat_history\n",
        "\n",
        "# Gradio chat interface with continuous output\n",
        "demo = gr.Interface(\n",
        "    fn=continuous_commentary,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"chatbot\",\n",
        "    description=\"Enter some text and see continuous commentary in chat format.\",\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
